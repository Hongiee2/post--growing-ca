<d-article>
<p>All multicellular organisms begin their life as a single egg cell. Being able to build their own bodies is probably the most fundamental skill every living creature possesses. Morphogenesis (the proocess of an organism’s shape development) is one of the most striking examples of self-organisation. Cells, tiny building blocks of bodies, communicate with their neighbors to decide the shape of organs and body plans, where to grow each organ, how to interconnect them, and when to eventually stop.</p>

<p>This process is extremely robust to perturbations. Even when the organism is fully developed, it still has the capability to repair damage - a process known as regeneration. Some creatures can fully regenerate vital organs, limbs, eyes, or even parts of the brain! Morphogenesis is a surprisingly adaptive process. Sometimes even a very atypical development process can result in a viable organism - for example, the parts of an early embryo cut into pieces will each form a complete individual - monozygotic twins!</p>

<p>Imagine if we could design systems of such plasticity and robustness: structures and machines that could grow and repair themselves. Biologists still do not fully understand the algorithms that enable cells to cooperate toward anatomical goals. To help crack the puzzle of the morphogenetic code, and also exploit the insights of biology to create self-repairing systems in real life, we try to replicate some of the desired properties in an <i>in silico</i> experiment.</p>

<p>The idea of modelling large-scale phenomena by simulating tiny local interactions found its way into engineering and scientific practice in the form of partial derivative equation systems (PDEs) and numerical methods for solving them, particle systems, and various kinds of Cellular Automata.</p>

<p>We will focus on Cellular Automata (CA) models. These typically consist of a grid of cells being iteratively updated, following the same set of rules applied to each cell on every step. The new state of a cell depends only on the states of cells in its tiny neighborhood. Despite the apparent simplicity, CAs often demonstrate rich, interesting behaviours, and have a long history of being applied to biological phenomena modeling.</p>

<p>Let’s try to develop a cellular automata update rule that, starting from a single cell, will produce a predefined multicellular pattern on a 2D grid. This is our analogous toy model of organism development. To design the CA, we must specify the possible cell states, and their update function. Typical CA models represent cell states with a set of discrete values, although variants using vectors of continuous values exist. The use of continuous values has the virtue of allowing the update rule to be a differentiable function of a cell’s neighbourhood state. The rules that guide individual cell behavior based on the local environment is analogous to the low-level hardware specification that is encoded by the genome of an organism. Running our model for a set amount of steps from a starting configuration will reveal the patterning behavior that is enabled by such hardware.</p>

<p>So - what is so special about differentiable update rules? They will allow us to use the powerful language of loss functions to express our wishes, and the developed machinery of gradient-based numerical optimization to fulfill them. The art of stacking together differentiable functions, and optimizing their parameters to perform various tasks, has a long history. In recent years it has flourished under various names, such as (Deep) Neural Networks, Deep Learning or Differentiable Programming.</p>
<h2>Model</h2>
<p><figure style="margin-left:auto; margin-right: auto; grid-column:page; width:100%; max-width:1200px;">
  <object data="figures/model.svg" type="image/svg+xml" style="width:100%"></object>
  <figcaption style="max-width:300px;">
  An illustration of the model used.
  </figcaption>
</figure></p>

<p><strong>Cell state</strong></p>
<p>We will represent each cell state as a vector of 16 real values (see the figure above). The first three channels represent the cell color visible to us (RGB). The target pattern has color channel values in range [0.0, 1.0] and an alpha equal to 1.0 for foreground pixels, and 0.0 for background. </p>

<p>The alpha channel (α) has a special meaning: it marks living cells, belonging to the currently grown pattern. In particular, cells having α > 0.1, and their neighbors, are considered “living”, while other cells are empty, and have all state values explicitly set to zero at every step. Thus cells with α<strong> </strong>> 0.1 can be thought of as “mature”, while their neighbors with alpha<=0.1 are “growing”, and can become mature if their alpha reaches the threshold.</p>

<p>Hidden channels don’t have a predefined meaning, and it’s up to the update rule to decide what to use them for. They can be interpreted as concentrations of some chemicals, electric potentials or some other signaling mechanism that are used by cells to orchestrate the growth. In terms of our biological analogy - all our cells share the same genome (update rule) and are only differentiated by the information encoded in their state vector.</p>

<p><strong>Cellular Automaton </strong><strong>rule</strong></p>
<p>Now it’s time to define the update rule. Our CA runs on a regular 2D grid of 16-dimensional vectors, essentially a 3D array of shape [height, width, 16]. We want to apply the same operation to each cell, and the result of this operation can only depend on the small (say 3x3) neighborhood of the cell. This is heavily reminiscent of the convolution operation, one of the cornerstones of signal processing and differential programming. Convolution is a linear operation, but it can be combined with other per-cell operations to produce a complex update rule, capable of learning the desired behaviour. Our cell update rule can be split into the following phases, applied in order:</p>

<ul><li><strong>Perception.</strong> This step defines what each cell perceives of the environment surrounding it. We implement this via a 3x3 convolution with a fixed kernel. One may argue that defining this kernel is superfluous - after all we could simply have the cell learn the requisite perception kernel coefficients. Our choice of fixed operations are motivated by the fact that real life cells often rely only on chemical gradients to guide the organism development. Thus, we are using classical Sobel filters to estimate the partial derivatives of cell state channels in x- and y- directions, forming a 2D gradient vector for each state channel. We concatenate those gradients with the cells own states, forming a 16*2+16=48 dimensional <i>perception vector</i>, or rather <i>percepted vector, </i>for each cell.</li></ul></ul>

<d-code block="" language="python">
<p>def perceive(state_grid):</p>
<p>  # Define the Sobel filters.</p>
<p>  sobel_x = [[-1, 0, +1],</p>
<p>             [-2, 0, +2],</p>
<p>             [-1, 0, +1]]</p>
<p>  sobel_y = transpose(sobel_x)</p>

<p>  # Convolve the Sobel filters </p>
<p>  # with the current states. </p>
<p>  # The convolution occurs in </p>
<p>  # the x, y and channel dimension.</p>
<p>  gradient_x = conv2d(sobel_x, state_grid)</p>
<p>  gradient_y = conv2d(sobel_y, state_grid)</p>
<p>  </p>
<p>  # Concatenate the cell's state channels, </p>
<p>  # the gradients of channels in x and </p>
<p>  # the gradient of channels in y. </p>
<p>  perception_grid = concat(</p>
<p>      state_grid, </p>
<p>      gradient_x, </p>
<p>      gradient_y, </p>
<p>      axis=0)</p>

<p>  return perception_grid</p>
</d-code>

<ul><li><strong>Update rule.</strong> Each cell now applies a series of operations to the perception vector, consisting of typical differentiable programming building blocks, such as 1x1-convolutions and ReLU nonlinearities, which we call the cell’s “update rule”. Recall that the update rule is learned, but every cell runs the same update rule. The network parametrizing this update rule consists of approximately 8,000 parameters. Inspired by residual neural networks, the update rule outputs an incremental update to the cell’s state, which applied to the cell before the next time step. The update rule is designed to exhibit “do-nothing” initial behaviour - implemented by initializing the weights of the final convolutional layer in the update rule with zero. We also forego applying a ReLU to the output of the last layer of the update rule as the incremental updates to the cell state must necessarily be able to both add or subtract from the state.</li></ul></ul>

<d-code block="" language="python">
<p># Thee following pseudocode operates on</p>
<p># a single cell’s perception vector. </p>
<p># Our reference implementation uses </p>
<p># 1D convolutions for performance reasons.</p>
<p>def update(perception_vector):</p>
<p>  x = dense(perception_vector, output_len=128)</p>
<p>  x = relu(x)</p>
<p>  ds = dense(x, output_len=16, weights_init=0.0)</p>
<p>  return ds</p>
</d-code>


<ul><li><strong>Stochastic cell update.</strong> Typical cellular automata update all cells simultaneously. This implies the existence of a global clock, synchronizing all cells. Relying on global synchronisation is not something one expects from a self-organising system. We relax this requirement by assuming that each cell performs an update independently, waiting for a random time interval between updates. To model this behaviour we apply a random per-cell mask to update vectors, setting all update values to zero with some predefined probability (we use 0.5 during training). This operation can be also seen as an application of per-cell dropout to update vectors.</li></ul></ul>

<d-code block="" language="python">
<p>def stochastic_update(state_grid, dy_grid):</p>
<p>  # Zero out a random fraction of the updates.</p>
<p>  rand_mask = cast(random(64, 64) < 0.5, float32)</p>
<p>  ds_grid = ds_grid * rand_mask</p>
<p>  return state_grid + ds_grid</p>
</d-code>

<ul><li><strong>Living cell masking.</strong> We want to model the growth process that starts with a single cell, and don’t want empty cells to participate in computations or carry any hidden state. We enforce this by explicitly setting all channels of empty cells to zeros. A cell is considered empty if there is no “mature” (alpha>0.1) cell in its 3x3 neightborhood.</li></ul></ul>

<d-code block="" language="python">
<p>def alive_masking(state_grid):</p>
<p>  # Take the alpha channel as the measure of "life".</p>
<p>  alive = max_pool(state_grid[:, :, 3], (3,3)) > 0.1</p>
<p>  state_grid = state_grid * cast(alive, float32)</p>
<p>  return state_grid</p>
</d-code>

<p>Todo: when images are completely finalized - compress + replace butterfly in SVG with pixel-based butterfly before final version.</p>



<h2>Experiment 1: Learning to Grow</h2>
<p><figure style="margin-left:auto; margin-right: auto; grid-column:page; width:100%; max-width:900px;">
  <object data="figures/training.svg" type="image/svg+xml" style="width:100%"></object>
  <figcaption>
  An illustration of the training algorithm. <span style="color:#e6ae87; font-weight: bold">Orange arrows</span> denote gradient flow. <span style="color:#666666; font-weight: bold">Dark grey arrows</span> denote forward pass flow.
  </figcaption>
</figure></p>

<p>We are going to initialize the grid with zeros, except a single seed cell in the center, which will have all channels except RGB set to one. We set RGB channels to zeros because we want the seed point to be with $α<strong> \leq</strong> 0.1$ visible on the white background.</p>

<p>Once the grid is initialized, we can iteratively apply the update rule. We sample a random number of CA steps from the [64, 96] range for each training step, as we want the pattern to be stable across a number of iterations. This should be a sufficient number of steps to grow the pattern of the size we work with (40x40), even considering the stochastic nature of our update rule.</p>

<p>At the last step we apply pixel-wise L2 loss between RGBA channels in the grid and the target pattern. This loss can be differentiably optimized with respect to the update rule parameters by backpropagation-over-time, the standard method of training recurrent neural networks.</p>

<p>Once the optimisation converges, we can run simulations to see how our learned CA grows the butterfly, starting from the seed cell. Let’s see what happens when we run it for longer than the number of steps used during training. The animation below shows the behaviour of a few different models, trained to generate different emoji patterns.</p>


<figure>    <video loop autoplay playsinline muted>
      <source src="figures/unstable.mp4" type="video/mp4">
      Your browser does not support the video tag.
    </video></figure>

<p>We can see that different training runs can lead to models with drastically different long term behaviours. Some tend to die out, some don’t seem to know how to stop growing, but some happen to be almost stable! How can we steer the training towards producing persistent patterns all the time?</p>
<h2>Experiment 2: What persists, exists</h2>
<p>One way of understanding why the previous experiment was unstable is to draw a parallel to dynamical systems. We can consider every cell to be a dynamical system, with each cell sharing the same dynamics, and all cells being locally coupled amongst themselves. When we train our cell update model - we are adjusting these dynamics. Our goal is to find dynamics for the cell such that the target pattern is an attractor and a fixed-point of the the overall coupled system. </p>

<p>Our first experiment encouraged the dynamics of the the coupled system to evolve in such a way as to reach the target pattern in about 64 to 96 steps, starting from a single black pixel (the seed). This likely defines an attractor acting only over the space of states visited between the seed and the target pattern. However we have likely not influenced the dynamics of the CA for the states it experiences shortly after 96 steps - it’s entirely possible the target pattern at this point is not a fixed point and that the dynamics result in a divergence to a previously unvisited state.</p>

<p>One strategy to solve this is letting the CA iterate for much a longer time and periodically applying the loss against the target, training the system by backpropagation through these longer time intervals. Intuitively we claim that with longer the time intervals and several applications of loss, the model is more likely to create an attractor for the target shape. However, longer time periods substantially increase the training time and more importantly, the memory requirements, given that the entire episode’s intermediate activations must be stored in memory for a backwards-pass to occur.</p>

<p>Instead, we propose a “sample pool” based strategy to a similar effect. We define a pool of seed states to start the iterations from, initially filled with the single black pixel seed state. We then sample a batch from this pool which we use in our training step. To prevent the equivalent of “catastrophic forgetting” we replace one sample in this batch with the original, single-pixel seed state. After concluding the training step , we replace samples in the pool that were sampled for the batch with the output states from the training step over this batch. The animation below shows all the entries in the pool every 100 training steps. </p>

<p>Early on in the training process, the random dynamics in the system allow the model to end up in various incomplete and incorrect states. As these states are sampled from the pool, we refine the dynamics to be able to recover from such states. Finally, as the model becomes more robust at going from a seed state to the target state, the samples in the pool reflect this and are more likely to be very close to the target pattern, allowing the training to refine these almost completed patterns further. </p>

<d-code block="" language="python">
<p>def pool_training():</p>
<p>  # Set alpha and hidden channels to (1.0).</p>
<p>  seed = zeros(64, 64, 16)</p>
<p>  seed[64//2, 64//2, 3:] = 1.0</p>
<p>  target = targets['butterfly']</p>
<p>  pool = [seed] * 1024</p>
<p>  for i in range(training_iterations):</p>
<p>    idxs, batch = pool.sample(32)</p>
<p>    # Sort by loss, descending.</p>
<p>    batch = sort_desc(batch, loss(batch))</p>
<p>    # Replace the highest-loss sample with seed.</p>
<p>    batch[0] = seed</p>
<p>    # Perform training.</p>
<p>    outputs, loss = train(batch, target)</p>
<p>    # Place outputs back in the pool.</p>
<p>    pool[idxs] = outputs </p>
</d-code>

<p>Essentially, we use the previous final states as new starting points to force our CA to learn how to persist or even improve an already formed pattern, in addition to being able to grow it from a seed. This makes it possible to add a periodical loss for significantly longer time intervals than otherwise possible, encouraging the generation of an attractor as the target shape in our coupled system. We also noticed that reseeding the highest loss sample in the batch, instead of a random one, makes training more stable at the initial stages, as it helps to clean up the low quality states from the pool.</p>

<p>Here is what a typical training progress of CA rule looks like. It learns to stabilize the pattern in parallel while refining its features.</p>

<figure>    <video loop autoplay playsinline muted>
      <source src="figures/train_steps_damage_0.mp4" type="video/mp4">
      Your browser does not support the video tag.
    </video>
</figure>
<p>CA behaviour at training steps 100, 500, 1000, 4000.</p>
<h2>Experiment 3: Learning to regenerate</h2>
<p>In addition to being able to grow their own bodies, living creatures are great at maintaining them. Not only worn out skin gets replaced with new, but very heavy damages to complex vital organs can be regenerated by some creatures. Is there a chance that some of the models we trained above have regenerative capabilities?</p>

<figure>    <video loop autoplay playsinline muted>
      <source src="figures/regen1.mp4" type="video/mp4">
      Your browser does not support the video tag.
    </video>
</figure>
<p>The animation above shows three different models trained using the same settings. We let each of models develop a pattern for 100 CA steps, and then damage it in five different ways: by removing different halves of the formed pattern, and by cutting out a square from the center. Once again, we see that these models show quite different out-of-training mode behaviour. “Model 3” has even developed quite strong regenerative capabilities, without being explicitly trained for it! </p>

<p>Since we trained our coupled system of cells to generate an attractor towards a target shape from a single cell, it was likely that these systems, once damaged, would generalize towards non self-destructive reactions. That’s because the systems were trained to grow, stabilize, and never entirely self-destruct. Some of these systems might naturally gravitate towards regenerative capabilities, but nothing stops them from developing different behaviors such as explosive mitoses (uncontrolled growth), unresponsiveness to damage (overstabilization), or even self destruction, especially for the more severe types of damage.</p>

<p>If we want our model to show more consistent and accurate regenerative capabilities, we can try to increase the basin of attraction for our attractor, that is the space of cell configurations that naturally gravitate towards our target shape. We will do this by damaging a few pool-sampled states before each training step. The systems will now also have to be able to regenerate from configurations damaged by random circles. Our hope is that it will generalize to regenerate from as many types of damage as possible.</p>

<figure>    <video loop autoplay playsinline muted>
      <source src="figures/batches.mp4" type="video/mp4">
      Your browser does not support the video tag.
    </video>
</figure>
<p>The animation above shows training progress, which includes sample damage. We sample 8 states from the pool. Then we replace the highest-loss sample (leftmost) with the seed, and damage three lowest-loss (rightmost) states by setting a random circular region to zeros. The bottom row shows states after CA iterations applied. These states get injected back into the pool.</p>

<figure>    <video loop autoplay playsinline muted>
      <source src="figures/regen2.mp4" type="video/mp4">
      Your browser does not support the video tag.
    </video>
</figure>
<p>As we can see from the animation above, models that were exposed to damages during training are much more robust, also for out of training types of damages.  </p>

<h2>Experiment 4: Rotating the perceptive field</h2>
<p>As previously described, we model the cell perception of neighbouring cells by estimating gradient vectors of state channels with Sobel filters. One can also think that each agent has two sensors that perceive directional derivatives of state vectors across its neighborhood along x and y axes. What happens if we rotate those sensors? We can do this by replacing Sobel kernels with their 2D rotation matrix-weighted counterparts.</p>
<figure><d-math block="">
  \begin{bmatrix}
  K_x \\
  K_y
  \end{bmatrix}
  =
  \begin{bmatrix}
  \cos \theta &  -\sin \theta \\
  \sin \theta &  \cos \theta
  \end{bmatrix} 
  *
  \begin{bmatrix}
  Sobel_x \\
  Sobel_y
  \end{bmatrix}
</d-math></figure>
<p>This simple modification of the perceptive field produces rotated versions of the pattern without retraining the model for any angle of choice!</p>

<p>In a perfect world, not quantized by individual cells in a lattice, this would not be too surprising, as, after all, one would expect the perceived gradients in $\vec{x}$ and $\vec{y}$ to be invariant to the chosen angle. However, successfully rotating the pattern in a pixel based model suggests that it’s very robust to perturbations outside of those which it has experienced during training, as rotating such a pattern on a 2D grid involves a mapping that’s not necessarily bijective and in most cases will involve interpolating.</p>

<figure><img src="figures/rotation.png"></figure>
<h2>WebGL Playground</h2>
<p>Starting from our first experiments on Neural CA growth and regeneration, we wanted to challenge our models with new situations not seen during training, like removing large portions of the pattern, or seeding multiple instances side-by-side. To facilitate exploration and sharing of our models, we created a TensorFlow.js playground that allowed us to interact with trained models right in a browser. The code for exporting and loading CA models in TF.js format is available in the accompanying Colab notebook.</p>

<p>Later during the work on this article, we decided to see how far we can push the performance and portability of this playground. We reimplemented all necessary operations from scratch using the WebGL API and GLSL shader language. This implementation powers the demo that can be found on the top of this page. We decided to quantize all model parameters and activations to 8-bit values, in order to maximize the performance and compatibility with mobile hardware.</p>

<p>The quantization was largely an afterthought, that was not accounted for during training. That’s why there are slight differences in models’ behaviour between the online demo and the python version. Yet, most of CAs that we’ve trained, managed to survive the strong quantization without severe artifacts, although in a few cases we had to resort to selecting the best model checkpoint between a few training runs.</p>

<p>We noticed that our models are more sensitive to the accuracy of small magnitude activation values, rather than large ones. That’s why we use non-linear $\arctan$ function to compress the unbounded activation values to the bounded segment, preserving the highest accuracy around zero.</p>

<p>Here is the detailed explanation of controls available in the demo on top of this page:</p>
<ul><li><strong>Target</strong> - selects one of a few models that is pre-trained to generate a specific image</li></ul>
<li><strong>Model type</strong> -<strong> </strong>selects between model variants that are trained following different procedures, described in the experiment sections 1-3.</li>
<li><strong>Rotation</strong> - controls the rotation applied to the perceived activation gradients, as described in the experiment section 4.</li>
<li><strong>Speed</strong> - simulation speed, from “paused” to “Full throttle”. “Full throttle” mode tries to load the GPU with as many iterations per second as possible. Use it cautiously, as it can result in reduction of system performance.</li></ul>
<h2>Glossary:</h2>
<p>### Let’s define the terms we use exactly such that we’re consistent throughout the paper and in future discussions.</p>

<ul><li><strong>Cell</strong> - A single pixel in the grid and its corresponding <strong>state</strong>.</li></ul>
<li><strong>(Cell) State - </strong>The internal 12/16 channels of state corresponding to each pixel. </li>
<li><strong>(Cell) Perception (Vector) - </strong>The combination of one's own state and neighbouring states. eyvind@: not a huge fan of this name (name not clear whether it alludes to what’s been “perceived” or some parameters deciding how the perception process works) but it’s similar to existing nomenclature such as “perceptive field” so I guess it stays. </li>
<li><strong>Hidden State</strong> - The portion of the Cell State which is <strong>not being rendered</strong>. eyvind@: again this is a bit tricky because it’s easily confused with hidden state in other contexts, such as RNNs and latent variable models.</li>
<li></li></ul>


<h2>Related work</h2>
<h3>Artificial Self-Organising systems</h3>
<h3>CA and PDEs</h3>
<p>There exists a giant body of literature that describes various flavours of cellular automata and PDE systems, and their applications to modelling physical, biological or even social systems. Albeit it would be impossible to present a just overview of this field in a few lines, we hereby describe some prominent examples that inspired this work.  Alan Turing introduced his famous Turing patterns back in 1952 <d-cite key="turing1990chemical"></d-cite>, suggesting how reaction-diffusion systems can be a valid modeling for chemical behaviors during morphogenesis. A particularly inspiring reaction-diffusion model that stood the test of time is the Gray-Scott model <d-cite key="Pearson189"></d-cite>, showing an extreme variety of behaviors controlled by just a few variables.</p>
<p>Todo add CA history.</p>

<p>[OLD]</p>
<p>There exists a giant body of literature that describes various flavours of cellular automata and PDE systems, and their applications to modelling physical, biological or even social systems. We will list a few prominent examples, that inspired this work:</p>

<ul><li>Von Neumann Cellular Automaton <d-cite key="10.5555/1102024"></d-cite></li></ul>
<li>Turing Patterns</li>
<li>Conway’s Game of Life</li>
<li>Gray-Scott Reaction-Diffusion</li>
<li>Wolfram’s A new kind of science <d-cite key="Wolfram2002ANK"></d-cite></li>
<li>SmoothLife <d-cite key="rafler2011generalization"></d-cite></li>
<li>Lenia - Biology of Artificial Life <d-cite key="Chan_2019"></d-cite></li></ul>

<p>A number of researchers used evolutionary algorithms to find CA rules that reproduce predefined simple patterns (<d-cite key="10.1007/978-3-642-19167-1_2"></d-cite>, <d-cite key="Nichele2018CANEATEC"></d-cite>) <u>1</u>, <u>2</u>). For example, J. Miller <d-cite key="Miller2004"></d-cite> proposed a very similar experiment setup, using evolutionary algorithms to design a CA rule that could build and regenerate the French flag, starting from a seed cell.</p>
<h3>Neural Networks and Self-Organisation</h3>
<p>The close relation between Convolutional Neural Networks and Cellular Automata was already observed by a number of researchers <d-cite key="wulff1993learning, Gilpin2018CellularAA"></d-cite>. The connection is so strong it allows to build Neural CA models using components readily available in popular ML frameworks. Thus, using a different jargon, our Neural CA can also be named Recurrent Residual Convolutional Networks.</p>

<p>Neural GPU <d-cite key="Kaiser2015NeuralGPU, Freivalds2017ImprovingTN"></d-cite> offers a computational architecture very similar to ours, but applied in the context of multiplication and sorting algorithm learning.</p>

<p>Looking more broadly, we think that the concept of self-organisation is finding its way into mainstream machine learning with popularisation of Graph Neural Network <d-cite key="wu2019comprehensive"></d-cite> models. Typically, GNNs run a repeated computation across vertices of a (possibly dynamic) graph. Vertices communicate locally through graph edges, and aggregate global information required to perform the task over multiple rounds of message exchanges. As if atoms talk to each other to figure out properties of a molecule <d-cite key="NIPS2015_5954"></d-cite>, or points of the point cloud talk to their neighbors to figure out their global shape <d-cite key="Wang_2019"></d-cite>.</p>
<h2>Discussion</h2>

</d-article>
<d-appendix>
<h3>Acknowledgments</h3>
<p>Michael Levin, Bert Chan, Mark Sandler, Blaise, Andrew Jackson (proofreading), Eyvind, Damien</p>

<h3>Author Contributions</h3>

<!-- References -->
<d-footnote-list></d-footnote-list>
<d-citation-list></d-citation-list>
<d-appendix>